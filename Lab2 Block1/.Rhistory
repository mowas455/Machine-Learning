m<-(-0.5*t(x))%*%(y)%*%(x)
w_oi<-m+log(z)
w_i<-y%*%x
#cat("w_0i",w_oi,"\n","w_i",w_i,"\n")
}
dc1<- decision_boundary(mean_group1,cv1,prior_group1)   #decision_bndy value of group 1
dc2<- decision_boundary(mean_group2,cv2,prior_group2)   #decision_bndy value of group 2
dc3<- decision_boundary(mean_group3,cv3,prior_group3)   #decision_bndy value of group 3
dc_b<-data.frame(dc1,dc2,dc3)
colnames(dc_b)<-c("Setosa","Versicolor","Virginica")
cat("Coefficients")
dc_b
#Bind the Discriminant values in the data.frame
d_val1<- disc_fn(x,pooled_cv,mean_group1,prior_group1)
d_val2<- disc_fn(x,pooled_cv,mean_group2,prior_group2)
d_val3<- disc_fn(x,pooled_cv,mean_group3,prior_group3)
disc_val<-cbind(d_val1,d_val2,d_val3)
dis<-as.data.frame(disc_val)
colnames(dis)<-c("setosa","versicolor","virginica")
# Found the target value of this data by max value occurs on the row
m<-colnames(dis)[max.col(dis, ties.method = "first")]
dis$y<-m     # add those values as y in the same dataframe
ggplot(data = data, aes(x = Sepal.Length,y = Sepal.Width)) +
geom_point(aes(color = factor(dis$y))) + ggtitle("Plot shows prediction using Discriminant Function")
#confusion matrix
cat("Confusion Matrix of prediction using Discriminant function")
table(as.factor(dis$y), y)
#miss-classification rate
missclassrate=function(y,y_i)
{
n=length(y)
v<-1-(sum(diag(table(y,y_i)))/n)
return(v)
}
ms=missclassrate(as.factor(dis$y),y)
cat("Misclassification Rate using Discrimiant function","\n")
ms
#LDA Model
fit_lda <- lda(y~., data = x)
# ca("coefficients")
# coef(fit_lda)
pred_lda <- predict(fit_lda, x)
vn<-data.frame(original = y, pred = pred_lda$class)
cat("Confusion Matrix using LDA model")
table(vn$pred,vn$original)
ldms=missclassrate(as.factor(vn$pred),y)
cat("Misclassification Rate using LDA model","\n")
ldms
ggplot(data = data, aes(x = Sepal.Length,y = Sepal.Width)) +
geom_point(aes(color = factor(vn$pred)))+ ggtitle("Plot shows prediction using LDA Model")
# bind the all groups in a single dataframe
bvn1 <- rmvnorm(50, mean = mean_group1, sigma = cv1 )
bvn2 <- rmvnorm(50, mean = mean_group2, sigma = cv2 )
bvn3 <- rmvnorm(50, mean = mean_group3, sigma = cv3 )
bvn<-rbind(bvn1,bvn2,bvn3)
bvn<-as.data.frame(bvn)
bvn$y<-y
sample_data<-bvn[sample(nrow(bvn), 150), ]
colnames(bvn)<-c("sepal.length","sepal.width","species")
ggplot(data =bvn, aes(x = sepal.length,y = sepal.width)) +
geom_point(aes(color = factor(y))) + ggtitle("sampled data of iris")
### 5.Logistic Regression Model
library(nnet)
irisModel<-multinom(y~x1+x2 ,data =  x)
m<-summary(irisModel)
pred2<-predict(irisModel,x)
pred2
ggplot(data = data, aes(x = Sepal.Length,y = Sepal.Width)) +
geom_point(aes(color = factor(pred2)))+ ggtitle("Plot shows prediction using Logistic Regression Model")
table("Prediction"=as.factor(pred2), "Reference"=y)   # confusiom Matrix
lms=missclassrate(as.factor(pred2),y)
cat("Misclassification Rate using Logistic Regression model","\n")
lms
# Discriminant function
disc_fn<-function(v,p_cv,m_g,p_g)
{
v<-as.matrix(v)
p_cv<-solve(p_cv)
d1<-((v%*%p_cv)%*%(m_g))
d2<-(0.5*t(m_g))%*%(p_cv)%*%(m_g)
t_d<-d1-(as.numeric(d2))+log(p_g)
return(t_d)
}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(MASS)
library(mvtnorm)
data=iris
x0=c(data$Sepal.Length)
y0=c(data$Sepal.Width)
z=c(data$Species)
x = data.frame(x1=x0,x2=y0)
y=as.factor(z)
# scatter plot for original data with target
ggplot(data = data, aes(x = Sepal.Length,y = Sepal.Width)) +geom_point(aes(color = factor(Species)))
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(MASS)
library(mvtnorm)
data=iris
x0=c(data$Sepal.Length)
y0=c(data$Sepal.Width)
z=c(data$Species)
x = data.frame(x1=x0,x2=y0)
y=as.factor(z)
# scatter plot for original data with target
ggplot(data = data, aes(x = Sepal.Length,y = Sepal.Width)) +geom_point(aes(color = factor(Species)))
# Grouping the Targets
group1_index = which( y == 1 )
group2_index = which( y == 2 )
group3_index = which( y == 3 )
#priors:
prior_group1 = length(group1_index) / length(y)
prior_group2 = length(group2_index) / length(y)
prior_group3 = length(group3_index) / length(y)
print("Prior probabilities of groups:")
print(c(prior_group1, prior_group2,prior_group3))
#means:
mean_group1 = as.matrix(colMeans(x[group1_index, ]))
mean_group2 = as.matrix(colMeans(x[group2_index, ]))
mean_group3 = as.matrix(colMeans(x[group3_index, ]))
print("Group means:")
print(cbind(mean_group1, mean_group2,mean_group3))
# Covariance Matrix
cv1<-cov(x[group1_index, ])
cv2<-cov(x[group2_index, ])
cv3<-cov(x[group3_index, ])
print("covariance matrix of group 1")
print(cv1)
print("covariance matrix of group 2")
print(cv2)
print("covariance matrix of group 3")
print(cv3)
# Pooled Co-variance Matrix
pooled_cv<-as.matrix((length(group1_index)*cv1)+(length(group2_index)*cv2)+
(length(group3_index)*cv3))/length(y)
print("Pooled covariance matrix of groups")
print(pooled_cv)
# Discriminant function
disc_fn<-function(v,p_cv,m_g,p_g)
{
v<-as.matrix(v)
p_cv<-solve(p_cv)
d1<-((v%*%p_cv)%*%(m_g))
d2<-(0.5*t(m_g))%*%(p_cv)%*%(m_g)
t_d<-d1-(as.numeric(d2))+log(p_g)
return(t_d)
}
# Decision Boundary Function
decision_boundary<-function(x,y,z)
{
y<-solve(y)
m<-(-0.5*t(x))%*%(y)%*%(x)
w_oi<-m+log(z)
w_i<-y%*%x
#cat("w_0i",w_oi,"\n","w_i",w_i,"\n")
}
dc1<- decision_boundary(mean_group1,cv1,prior_group1)   #decision_bndy value of group 1
dc2<- decision_boundary(mean_group2,cv2,prior_group2)   #decision_bndy value of group 2
dc3<- decision_boundary(mean_group3,cv3,prior_group3)   #decision_bndy value of group 3
dc_b<-data.frame(dc1,dc2,dc3)
colnames(dc_b)<-c("Setosa","Versicolor","Virginica")
cat("Coefficients")
dc_b
#Bind the Discriminant values in the data.frame
d_val1<- disc_fn(x,pooled_cv,mean_group1,prior_group1)
d_val2<- disc_fn(x,pooled_cv,mean_group2,prior_group2)
d_val3<- disc_fn(x,pooled_cv,mean_group3,prior_group3)
disc_val<-cbind(d_val1,d_val2,d_val3)
dis<-as.data.frame(disc_val)
colnames(dis)<-c("setosa","versicolor","virginica")
# Found the target value of this data by max value occurs on the row
m<-colnames(dis)[max.col(dis, ties.method = "first")]
dis$y<-m     # add those values as y in the same dataframe
ggplot(data = data, aes(x = Sepal.Length,y = Sepal.Width)) +
geom_point(aes(color = factor(dis$y))) + ggtitle("Plot shows prediction using Discriminant Function")
#confusion matrix
cat("Confusion Matrix of prediction using Discriminant function")
table(as.factor(dis$y), y)
#miss-classification rate
missclassrate=function(y,y_i)
{
n=length(y)
v<-1-(sum(diag(table(y,y_i)))/n)
return(v)
}
ms=missclassrate(as.factor(dis$y),y)
cat("Misclassification Rate using Discrimiant function","\n")
ms
#LDA Model
fit_lda <- lda(y~., data = x)
# ca("coefficients")
# coef(fit_lda)
pred_lda <- predict(fit_lda, x)
vn<-data.frame(original = y, pred = pred_lda$class)
cat("Confusion Matrix using LDA model")
table(vn$pred,vn$original)
ldms=missclassrate(as.factor(vn$pred),y)
cat("Misclassification Rate using LDA model","\n")
ldms
ggplot(data = data, aes(x = Sepal.Length,y = Sepal.Width)) +
geom_point(aes(color = factor(vn$pred)))+ ggtitle("Plot shows prediction using LDA Model")
# bind the all groups in a single dataframe
bvn1 <- rmvnorm(50, mean = mean_group1, sigma = cv1 )
bvn2 <- rmvnorm(50, mean = mean_group2, sigma = cv2 )
bvn3 <- rmvnorm(50, mean = mean_group3, sigma = cv3 )
bvn<-rbind(bvn1,bvn2,bvn3)
bvn<-as.data.frame(bvn)
bvn$y<-y
sample_data<-bvn[sample(nrow(bvn), 150), ]
colnames(bvn)<-c("sepal.length","sepal.width","species")
ggplot(data =bvn, aes(x = sepal.length,y = sepal.width)) +
geom_point(aes(color = factor(y))) + ggtitle("sampled data of iris")
### 5.Logistic Regression Model
library(nnet)
irisModel<-multinom(y~x1+x2 ,data =  x)
m<-summary(irisModel)
pred2<-predict(irisModel,x)
pred2
ggplot(data = data, aes(x = Sepal.Length,y = Sepal.Width)) +
geom_point(aes(color = factor(pred2)))+ ggtitle("Plot shows prediction using Logistic Regression Model")
table("Prediction"=as.factor(pred2), "Reference"=y)   # confusiom Matrix
lms=missclassrate(as.factor(pred2),y)
cat("Misclassification Rate using Logistic Regression model","\n")
lms
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(glmnet)
library(psych)
library(tidyr)
library(kknn)
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(glmnet)
library(psych)
library(tidyr)
library(kknn)
#Load Data
dataset = read.csv("C:/Users/Mounish/Desktop/ML/Lab1 Block 1/optdigits.csv")
cat("Dimension of Dataset is",dim(dataset),"\n")
#Split the data into train, validation and test
n <- dim(dataset)[1]
set.seed(12345)
id <- sample(1:n, floor(n*0.5))
train <- dataset[id,]
id1 <- setdiff(1:n, id)
set.seed(12345)
id2 <- sample(id1, floor(n*0.25))
validation<- dataset[id2,]
id3 <- setdiff(id1,id2)
test <- dataset[id3,]
knn.fit <- kknn(as.factor(X0.26)~., train=train,test=train ,k = 30,
kernel = "rectangular")
knn.fit_v <- kknn(as.factor(X0.26)~., train=validation,test=validation,k = 30,
kernel = "rectangular")
knn.fit_t <- kknn(as.factor(X0.26)~., train=train,test=test,k = 30,
kernel = "rectangular")
pred.knn <- fitted(knn.fit)
pred.knn_v <- fitted(knn.fit_v)
pred.knn_t <- fitted(knn.fit_t)
c<-table(train$X0.26,as.factor(pred.knn)) # for train data
cat("Confusion matrix for the training data.")
print(c)
v<-table(test$X0.26,as.factor(pred.knn_t)) # for test data
cat("Confusion matrix for the test data.")
print(v)
missclassrate=function(y,y_i)
{
n=length(y)
v<-1-(sum(diag(table(y,y_i)))/n)
return(v)
}
missclassrate(train$X0.26,as.factor(pred.knn)) # for training data
missclassrate(test$X0.26,as.factor(pred.knn_t)) # for test data
v=data.frame(knn.fit$prob)
estm_pb <- colnames(v)[apply(v, 1, which.max)]
v$y<-train$X0.26
v$fit <- knn.fit$fitted
v$estm_pb <- estm_pb
###
y_8 <- v[v$y == 8,]
yhat_8 <- y_8[y_8$fit == 8,]
###
# Easy
easy <- as.numeric(row.names(yhat_8[order(-yhat_8[,9]),][1:2,]))
# Tough
tougher <- as.numeric(row.names(yhat_8[order(yhat_8[,9]),][1:3,]))
col=heat.colors(12)
heatmap(t(matrix(unlist(train[easy[1],-65]), nrow=8)),Colv = NA, Rowv = NA,col=rev(heat.colors(12)))
heatmap(t(matrix(unlist(train[easy[2],-65]), nrow=8)), Colv = NA, Rowv = NA,col=rev(heat.colors(12)))
heatmap(t(matrix(unlist(train[tougher[1],-65]), nrow=8)), Colv = NA, Rowv = NA,col=rev(heat.colors(12)))
heatmap(t(matrix(unlist(train[tougher[2],-65]), nrow=8)), Colv = NA, Rowv = NA,col=rev(heat.colors(12)))
heatmap(t(matrix(unlist(train[tougher[3],-65]), nrow=8)), Colv = NA, Rowv = NA,col=rev(heat.colors(12)))
k=1
k.optm=c()
y.optm=c()
for (i in 1:30){
knn.fit <-kknn(as.factor(X0.26)~., train=train,test=train, k = i,kernel = "rectangular")
knn.fit_v <-kknn(as.factor(X0.26)~., train=train,test=validation, k = i,kernel = "rectangular")
ypred = fitted(knn.fit)
vpred = fitted(knn.fit_v)
k.optm[i] = 1-(sum(diag(as.matrix(table(Actual = train$X0.26, Predicted = ypred))))/nrow(train))
y.optm[i] = 1-(sum(diag(as.matrix(table(Actual = validation$X0.26, Predicted = vpred))))/nrow(validation))
}
k.optm
y.optm
my.df  <- data.frame(K_Value = c(1:30), Training= c(k.optm), Validation = c(y.optm))
plot3<-ggplot( ) +
geom_line(aes(x=my.df$K_Value,y=my.df$Validation,colour="green")) +
geom_line(aes(x=my.df$K_Value,y=my.df$Training,colour="red"))+
ylab("Missclassification Rate ") +xlab("K_value")+
scale_color_manual(name = "Missclassification Rate", labels = c("Validation ", "Training "),
values =c("green", "red"))
print(plot3)
optm_value_k <- which(y.optm == min(y.optm))
cat("calculated optimum value of K is: ", optm_value_k)
# Test Error for the model using optimal K value
kknn_test <- kknn(as.factor(X0.26)~., train = train, test = test,
k = optm_value_k, kernel = "rectangular")
testpred <- fitted(kknn_test)
missclasstest <- missclassrate(test$X0.26,as.factor(testpred))
#missclasstest
cat("Missclassification rate of test error using optimal K value",missclasstest)
rp <- function(i){
n <- rep(0,10)
n[i+1] <- 1
return(I(n))
}
er<-c()
for (i in 1:30){
knn.fit <-kknn(as.factor(X0.26)~., train=train,test=validation, k = i,kernel = "rectangular")
x<- data.frame(knn.fit$prob)
#max_prob <- colnames(x)[apply(x ,1,which.max)]
x$target <- validation$X0.26
x$fit <- knn.fit$fitted
#x$max_prob <- max_prob
x$binary <- (lapply(as.numeric(x$target)-1, rp))
#cross entropy loss
for (j in 1:nrow(x)){
x[j, "cross_entropy"] <- -sum(log(x[j,1:10]+1e-15)* x[[j, "binary"]])
}
er[i] <- mean(x$cross_entropy)
}
er
df<-data.frame(cross_entropy=er,k_Value=c(1:30))
plot4<-ggplot(df,aes(x=k_Value,y=cross_entropy,col="red" ))  +
geom_line()+ggtitle("Empirical Risk Of calculating K_value")
print(plot4)
best_optm_k_value <-which.min(er)
cat("calculated optimum value of K by using the cross entropy function is: ", best_optm_k_value)
dataset = read.csv("C:/Users/Mounish/Desktop/ML/Lab1 Block 1/parkinsons.csv")
# Q1
p<-dataset$motor_UPDRS
hist(p,main="Distribution of Y_hat")
n <- dim(dataset)[1]
set.seed(12345)
id <- sample(1:n, floor(n*0.6))  #Training split
train <- dataset[id,]
id1 <- setdiff(1:n, id)         # Setting different data b/w Trainig and test
set.seed(12345)
id2 <- sample(id1, floor(n*0.4))
test<- dataset[id2,]
dim(train)
dim(test)
# Scale the data
train<-scale(train)
exclude_x_columns = c("subject.","sex","age","test_time","motor_UPDRS","total_UPDRS")
x_train= subset(train, select = -c(subject.,sex,age,test_time,motor_UPDRS,total_UPDRS))
y_train=train[,5]
wi = as.matrix(rnorm(dim(x_train)[2], mean = 0, sd = 0.01))
sigma_i = runif(1, 0.001, 1)
llhood<-function(x,y,w,sigma){
beta=as.matrix(w)
sigma=sigma
x=as.matrix(x)
y=as.matrix(y)
n<-nrow(x)
RSS <- sum(( y - (x%???% beta) )^2)
ll<-(-(n/2)*log(2*pi*sigma^2)-(1/(2*sigma^2))*RSS)
return(ll)
}
llhood(x_train,y_train,wi,sigma_i)
llhood<-function(x,y,w,sigma){
beta=as.matrix(w)
sigma=sigma
x=as.matrix(x)
y=as.matrix(y)
n<-nrow(x)
RSS <- sum(( y - (x%*% beta) )^2)
ll<-(-(n/2)*log(2*pi*sigma^2)-(1/(2*sigma^2))*RSS)
return(ll)
}
llhood(x_train,y_train,wi,sigma_i)
#ridge function
ridge<- function(x,y,w,sigma,lamda)
{
beta=as.matrix(w)
sigma=sigma
x=as.matrix(x)
y=as.matrix(y)
n<-nrow(x)
RSS <- sum(( y - (x%???% beta) )^2)
penalty<-lamda*sum(beta  ^2)
ll1<--((n/2)*log(2*pi*sigma^2))-((1/(2*sigma^2))*RSS)+penalty
return(ll1)
}
ridge(x_train,y_train,wi,sigma_i,1)
#ridge function
ridge<- function(x,y,w,sigma,lamda)
{
beta=as.matrix(w)
sigma=sigma
x=as.matrix(x)
y=as.matrix(y)
n<-nrow(x)
RSS <- sum(( y - (x%*% beta) )^2)
penalty<-lamda*sum(beta  ^2)
ll1<--((n/2)*log(2*pi*sigma^2))-((1/(2*sigma^2))*RSS)+penalty
return(ll1)
}
ridge(x_train,y_train,wi,sigma_i,1)
sum(wi)
#=============================================================================================================
# Using Package
library(tidyverse)
library(broom)
library(glmnet)
n <- dim(dataset)[1]
set.seed(12345)
id <- sample(1:n, floor(n*0.6))  #Training split
train <- dataset[id,]
id1 <- setdiff(1:n, id)         # Setting different data b/w Trainig and test
set.seed(12345)
id2 <- sample(id1, floor(n*0.4))
test<- dataset[id2,]
dim(train)
dim(test)
# Scale the data
train<-scale(train)
exclude_x_columns = c("subject.","sex","age","test_time","motor_UPDRS","total_UPDRS")
x_train= subset(train, select = -c(subject.,sex,age,test_time,motor_UPDRS,total_UPDRS))
y_train=train[,5]
#lamda Value
lambdas <- 10^seq(3, -2, by = -.1)
#Ridge Model
fit <- glmnet(x_train, y_train, alpha = 0, lambda = lambdas)
summary(fit)
cv_fit <- cv.glmnet(x_train, y_train, alpha = 0, lambda = lambdas)
plot(cv_fit)
plot(fit)
opt_lambda <- cv_fit$lambda.min
opt_lambda
y_predicted <- predict(fit, s = opt_lambda, newx = x_train)
# Sum of Squares Total and Error
sst <- sum((y_train - mean(y_train))^2)
sse <- sum((y_predicted - y_train)^2)
# R squared
rsq <- 1 - sse / sst
rsq
#libraries
library(ggplot2)
library(glmnet)
library(psych)
library(tidyr)
# Read and split the data
data = read.csv("C:/Users/Mounish/Desktop/ML/Lab1 Block 1/tecator.csv")
# Split data into train and test set
n = dim(data)[1]
set.seed(12345)
id = sample(1:n, floor(n*0.5))
train = data[id,]
test = data[-id,]
#=================================================================================================================
# 1.fit the linear model to the this data
fit<-lm(Fat~ .-Ã¯..Sample-Protein-Moisture,data=train)
summary(fit)
lm_train_pred = predict(fit, train)
train_mse = mean((train$Fat-lm_train_pred)^2)
train_rmse = sqrt(mean((train$Fat-lm_train_pred)^2))
test_predictions_lm = predict(fit, test)
test_mse = mean((test$Fat-test_predictions_lm)^2)
test_rmse = sqrt(mean((test$Fat-test_predictions_lm)^2))
cat("Train error\nMSE: ", train_mse, "\nRMSE: ", train_rmse,"\n\nTest error\nMSE: ", test_mse, "\nRMSE: ", test_rmse)
#================================================================================================================
# 3.Fit The Lasso model
covariates = scale( train[,2:101])
response = scale(train$Fat)
set.seed(12345)
model_lasso = glmnet(as.matrix(covariates), response, alpha=1, family="gaussian")
plot(model_lasso, xvar="lambda", label=T)
10^
#================================================================================================================
# 4.Presenting a plot that represent how DOF depend on the penalty parameter.
plot(model_lasso$lambda,model_lasso$df,col="red",
ylab="Degrees of Freedom",
xlab="Lambda")
#By the decreasing degrees of freedom , the lambda value of the model get decreased, but the
#=================================================================================================================
#5.Fit the ridge model.
model_ridge = glmnet(as.matrix(covariates), response, alpha=0, family="gaussian")
plot(model_ridge, xvar="lambda", label=T)
#=========================================================================================================================
# 6.Optimal Lasso
set.seed(12345)
cv_model1<-cv.glmnet(as.matrix(covariates),response,family="gaussian" , lambda=10^seq(-5,5,length=500), alpha=1)
plot(cv_model1)
lambda_optimal<-cv_model1$lambda.1se
lambda_optimal
#Finding the optimal features
features<-as.matrix(coef(cv_model1))
optimal_features<-features[features!=0,]
length(optimal_features)
