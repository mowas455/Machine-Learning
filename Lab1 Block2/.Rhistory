n<-c(1,10,100)
acc = c()
times=c(1:1000)
for(i in times){
temp<-trdata[sample(nrow(trdata), 100), ]
for(j in n){
rf=randomForest(trlabels ~.,data=temp,ntree=j,nodesize=25,keep.forest = TRUE)
pred=predict(rf,newdata = temp)
acc[j] = mean(trlabels == pred)
}
}
length(acc)
for(i in times){
temp<-trdata[sample(nrow(trdata), 100), ]
for(j in n){
w = length(acc)
rf=randomForest(trlabels ~.,data=temp,ntree=j,nodesize=25,keep.forest = TRUE)
pred=predict(rf,newdata = temp)
acc[w+1] = mean(trlabels == pred)
}
}
length(acc)
n<-c(1,10,100)
acc = c()
times=c(1:1000)
for(i in times){
temp<-trdata[sample(nrow(trdata), 100), ]
for(j in n){
w = length(acc)
rf=randomForest(trlabels ~.,data=temp,ntree=j,nodesize=25,keep.forest = TRUE)
pred=predict(rf,newdata = temp)
acc[w+1] = mean(trlabels == pred)
}
}
length(acc)
n<-c(1,10,100)
acc = c()
times=c(1:1000)
for(i in times){
temp<-trdata[sample(nrow(trdata), 100), ]
for(j in n){
w = length(acc)
rf=randomForest(trlabels ~.,data=temp,ntree=j,nodesize=25,keep.forest = TRUE)
pred=predict(rf,newdata = temp)
acc[w+1] = mean(trlabels == pred)
print(acc[w+1])
}
}
setwd("C:/Users/Mounish/Desktop/ML/lab1")
setwd("C:/Users/Mounish/Desktop/ML/Lab1 Block 1")
dataset = read.csv("C:/Users/Mounish/Desktop/ML/Lab1/optdigits.csv")
dataset = read.csv("C:/Users/Mounish/Desktop/ML/Lab1 Block 1/optdigits.csv")
setwd("C:/Users/Mounish/Desktop/ML/Lab1 Block2")
set.seed(1234567890)
max_it <- 100 # max number of EM iterations
min_change <- 0.1 # min change in log likelihood between two consecutive EM iterations
N=1000 # number of training points
D=10 # number of dimensions
x <- matrix(nrow=N, ncol=D) # training data
true_pi <- vector(length = 3) # true mixing coefficients
true_mu <- matrix(nrow=3, ncol=D) # true conditional distributions
true_pi=c(1/3, 1/3, 1/3)
true_mu[1,]=c(0.5,0.6,0.4,0.7,0.3,0.8,0.2,0.9,0.1,1)
true_mu[2,]=c(0.5,0.4,0.6,0.3,0.7,0.2,0.8,0.1,0.9,0)
true_mu[3,]=c(0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5)
plot(true_mu[1,], type="o", col="blue", ylim=c(0,1))
points(true_mu[2,], type="o", col="red")
points(true_mu[3,], type="o", col="green")
# Producing the training data
for(n in 1:N) {
k <- sample(1:3,1,prob=true_pi)
for(d in 1:D) {
x[n,d] <- rbinom(1,1,true_mu[k,d])
}
}
EM <- function(k){
set.seed(123456789)
K <- k
z <- matrix(nrow=N, ncol=K) # fractional component assignments
pi <- vector(length = K) # mixing coefficients
mu <- matrix(nrow=K, ncol=D) # conditional distributions
llik <- vector(length = max_it) # log likelihood of the EM iterations
# Random initialization of the parameters
pi <- runif(K,0.49,0.51)
pi <- pi / sum(pi)
for(k in 1:K) {
mu[k,] <- runif(D,0.49,0.51)
}
colors <- c(’red’,’blue’,’green’,’yellow’)
for(it in 1:max_it) {
# E-step: Computation of the fractional component assignments
for(n in 1:N){
for(k in 1:K){
z[n,k] <- pi[k] %*% prod((mu[k,] ^ x[n,]) * ((1-mu[k,]) ^ (1 - x[n,])))
}
}
z <- z / rowSums(z)
n_k <- colSums(z)
x_mean_k <- (t(z) %*% x)/n_k
# Log Likelihood Calculation
#llik[it] <- sum(t(z) %*% (matrix(log(pi),ncol=K, nrow=N, byrow=TRUE)+
#(x %*% t(log(mu)) + (1-x) %*% t(log(1-mu)))))
llk = 0
for(i in 1:N){
for(j in 1:K){
llk_inner = 0
for(d in 1:D){
llk_inner = llk_inner+(x[i,d]*log(mu[j,d])+(1-x[i,d])*log(1-mu[j,d]))
}
llk = llk+(z[i,j]) * (log(pi[j])+llk_inner)
}
}
llik[it] = llk
#cat("iteration: ", it, "log likel
#flush.console()
# Stop if the log likelihood has not changed significantly
if(it > 1){
if((llik[it] - llik[it-1]) <= 0.1){
break
}
}
#M-step: ML parameter estimation from the data and fractional component assignments
mu <- x_mean_k
pi <- n_k/sum(n_k)
}
print(pi)
print(mu)
plot(mu[1,], type="o", col=colors[1], ylim=c(0,1))
for(p in 2:K){
points(mu[p,], type="o",col = colors[p])
}
plot(llik[1:it], type="o")
}
trees <- c(1,10,100)
forests_three <- list(list(list()))
set.seed(12345)
for (set in 1:1000) {
forests_three[[set]] <- list()
for (ntrees in 1:3) {
forests_three[[set]][[ntrees]] <- randomForest(x = datasets[[set]]$x,
y = datasets[[set]]$y,
xtest = tedata,
ytest = telabels,
ntree = trees[ntrees],
nodesize = 12,
keep.forest = TRUE)
}
}
EM(2)
EM(3)
EM(4)
EM <- function(k){
set.seed(123456789)
K <- k
z <- matrix(nrow=N, ncol=K) # fractional component assignments
pi <- vector(length = K) # mixing coefficients
mu <- matrix(nrow=K, ncol=D) # conditional distributions
llik <- vector(length = max_it) # log likelihood of the EM iterations
# Random initialization of the parameters
pi <- runif(K,0.49,0.51)
pi <- pi / sum(pi)
for(k in 1:K) {
mu[k,] <- runif(D,0.49,0.51)
}
colors <- c(’red’,’blue’,’green’,’yellow’)
for(it in 1:max_it) {
# E-step: Computation of the fractional component assignments
for(n in 1:N){
for(k in 1:K){
z[n,k] <- pi[k] %*% prod((mu[k,] ^ x[n,]) * ((1-mu[k,]) ^ (1 - x[n,])))
}
}
z <- z / rowSums(z)
n_k <- colSums(z)
x_mean_k <- (t(z) %*% x)/n_k
# Log Likelihood Calculation
#llik[it] <- sum(t(z) %*% (matrix(log(pi),ncol=K, nrow=N, byrow=TRUE)+
#(x %*% t(log(mu)) + (1-x) %*% t(log(1-mu)))))
llk = 0
for(i in 1:N){
for(j in 1:K){
llk_inner = 0
for(d in 1:D){
llk_inner = llk_inner+(x[i,d]*log(mu[j,d])+(1-x[i,d])*log(1-mu[j,d]))
}
llk = llk+(z[i,j]) * (log(pi[j])+llk_inner)
}
}
llik[it] = llk
#cat("iteration: ", it, "log likel
#flush.console()
# Stop if the log likelihood has not changed significantly
if(it > 1){
if((llik[it] - llik[it-1]) <= 0.1){
break
}
}
#M-step: ML parameter estimation from the data and fractional component assignments
mu <- x_mean_k
pi <- n_k/sum(n_k)
}
print(pi)
print(mu)
plot(mu[1,], type="o", col=colors[1], ylim=c(0,1))
for(p in 2:K){
points(mu[p,], type="o",col = colors[p])
}
plot(llik[1:it], type="o")
}
trees <- c(1,10,100)
forests_three <- list(list(list()))
set.seed(12345)
for (set in 1:1000) {
forests_three[[set]] <- list()
for (ntrees in 1:3) {
forests_three[[set]][[ntrees]] <- randomForest(x = datasets[[set]]$x,
y = datasets[[set]]$y,
xtest = tedata,
ytest = telabels,
ntree = trees[ntrees],
nodesize = 12,
keep.forest = TRUE)
}
}
EM(2)
EM(3)
EM(4)
set.seed(1234567890)
max_it <- 100 # max number of EM iterations
min_change <- 0.1 # min change in log likelihood between two consecutive EM iterations
N=1000 # number of training points
D=10 # number of dimensions
x <- matrix(nrow=N, ncol=D) # training data
x
true_pi <- vector(length = 3) # true mixing coefficients
true_mu <- matrix(nrow=3, ncol=D) # true conditional distributions
true_pi=c(1/3, 1/3, 1/3)
true_mu[1,]=c(0.5,0.6,0.4,0.7,0.3,0.8,0.2,0.9,0.1,1)
true_mu[2,]=c(0.5,0.4,0.6,0.3,0.7,0.2,0.8,0.1,0.9,0)
true_mu[3,]=c(0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5)
plot(true_mu[1,], type="o", col="blue", ylim=c(0,1))
points(true_mu[2,], type="o", col="red")
points(true_mu[3,], type="o", col="green")
# Producing the training data
for(n in 1:N) {
k <- sample(1:3,1,prob=true_pi)
for(d in 1:D) {
x[n,d] <- rbinom(1,1,true_mu[k,d])
}
}
k
set.seed(1234567890)
max_it <- 100 # max number of EM iterations
min_change <- 0.1 # min change in log likelihood between two consecutive EM iterations
N=1000 # number of training points
D=10 # number of dimensions
x <- matrix(nrow=N, ncol=D) # training data
true_pi <- vector(length = 3) # true mixing coefficients
true_mu <- matrix(nrow=3, ncol=D) # true conditional distributions
true_pi=c(1/3, 1/3, 1/3)
true_mu[1,]=c(0.5,0.6,0.4,0.7,0.3,0.8,0.2,0.9,0.1,1)
true_mu[2,]=c(0.5,0.4,0.6,0.3,0.7,0.2,0.8,0.1,0.9,0)
true_mu[3,]=c(0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5)
plot(true_mu[1,], type="o", col="blue", ylim=c(0,1))
points(true_mu[2,], type="o", col="red")
points(true_mu[3,], type="o", col="green")
# Producing the training data
for(n in 1:N) {
k <- sample(1:3,1,prob=true_pi)
for(d in 1:D) {
x[n,d] <- rbinom(1,1,true_mu[k,d])
}
}
K=3 # number of guessed components
z <- matrix(nrow=N, ncol=K) # fractional component assignments
pi <- vector(length = K) # mixing coefficients
mu <- matrix(nrow=K, ncol=D) # conditional distributions
llik <- vector(length = max_it) # log likelihood of the EM iterations
# Random initialization of the paramters
pi <- runif(K,0.49,0.51)
pi <- pi / sum(pi)
for(k in 1:K) {
mu[k,] <- runif(D,0.49,0.51)
}
pi
mu
for(it in 1:max_it) {
points(mu[2,], type="o", col="red")
points(mu[3,], type="o", col="green")
#points(mu[4,], type="o", col="yellow")
Sys.sleep(0.5)
# E−s t e p : Computat ion o f t h e f r a c t i o n a l component a s s ig nme n t s
bp <- exp( x %∗% log ( t (mu) ) + (1-x ) %∗% log (1-t (mu) ) )
pim <- matrix( rep( pi ,N) , nrow=N, ncol=K)
px <- bp ∗ p m
z <- px / rowSums ( px )
pm <- matrix( rep( pi ,N) , nrow=N, ncol=K)
px <- bp ∗ pm
# E−s t e p : Computat ion o f t h e f r a c t i o n a l component a s s ig nme n t s
bp <- exp(x %∗% log ( t (mu) ) + (1-x ) %∗% log (1-t (mu) ) )
pm <- matrix( rep( pi ,N) , nrow=N, ncol=K)
px <- bp∗ pm
z <- px / rowSums ( px )
z
#Log l i k e l i h o o d compu ta t ion .
lhood <− x %∗% log ( t (mu) ) + (1−x ) %∗% log(1−t (mu) )
for (n in 1 :N){
for ( k in 1 :K){
llik [ it ] <− llik [ it ] + z [ n , k ] ∗ lhood [ n , k ] + log ( pi [ k ] )
}
}
set.seed(1234567890)
max_it <- 100 # max number of EM iterations
min_change <- 0.1 # min change in log likelihood between two consecutive EM iterations
N=1000 # number of training points
D=10 # number of dimensions
x <- matrix(nrow=N, ncol=D) # training data
true_pi <- vector(length = 3) # true mixing coefficients
true_mu <- matrix(nrow=3, ncol=D) # true conditional distributions
true_pi=c(1/3, 1/3, 1/3)
true_mu[1,]=c(0.5,0.6,0.4,0.7,0.3,0.8,0.2,0.9,0.1,1)
true_mu[2,]=c(0.5,0.4,0.6,0.3,0.7,0.2,0.8,0.1,0.9,0)
true_mu[3,]=c(0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5)
plot(true_mu[1,], type="o", col="blue", ylim=c(0,1))
points(true_mu[2,], type="o", col="red")
points(true_mu[3,], type="o", col="green")
# Producing the training data
for(n in 1:N) {
k <- sample(1:3,1,prob=true_pi)
for(d in 1:D) {
x[n,d] <- rbinom(1,1,true_mu[k,d])
}
}
K=3 # number of guessed components
z <- matrix(nrow=N, ncol=K) # fractional component assignments
pi <- vector(length = K) # mixing coefficients
mu <- matrix(nrow=K, ncol=D) # conditional distributions
llik <- vector(length = max_it) # log likelihood of the EM iterations
# Random initialization of the paramters
pi <- runif(K,0.49,0.51)
pi <- pi / sum(pi)
for(k in 1:K) {
mu[k,] <- runif(D,0.49,0.51)
}
pi
mu
for(it in 1:max_it) {
plot(mu[1,], type="o", col="blue", ylim=c(0,1))
points(mu[2,], type="o", col="red")
points(mu[3,], type="o", col="green")
#points(mu[4,], type="o", col="yellow")
Sys.sleep(0.5)
# E−s t e p : Computat ion o f t h e f r a c t i o n a l component a s s ig nme n t s
bp <- exp(x %∗% log ( t (mu) ) + (1-x ) %∗% log (1-t (mu) ) )
pm <- matrix( rep( pi ,N) , nrow=N, ncol=K)
px <- bp∗ pm
z <- px / rowSums ( px )
#Log l i k e l i h o o d compu ta t ion .
lhood <− x %∗% log ( t (mu) ) + (1−x ) %∗% log(1−t (mu) )
for (n in 1 :N){
for ( k in 1 :K){
llik[it] <− llik [it] + z [n,k] ∗ lhood [n,k] + log (pi[k])
}
}
cat ( ” iteration : ” , it , ”loglikelihood:”, llik[it],”\n”)
flush.console()
ifelse( it>1 & llik[it] − llik[it−1]< minchange ,
stop ( ”EM has converged ” ) , llik[it] )
#M−s t e p : ML parame ter e s t im a t i o n from t h e d a t a and f r a c t i o n a l
component a s sig nm e n t s
cat (”iteration : ”, it , ”loglikelihood:”, llik[it],”\n”)
cat (”iteration : ”, it , ”loglikelihood:”, llik[it],”\n”)
cat (”iteration: ”, it , ”loglikelihood:”, llik[it],”\n”)
cat (”iteration:_”, it , ”loglikelihood:_”, llik[it],”\n”)
cat (”iteration”, it , ”loglikelihood:_”, llik[it],”\n”)
cat ('iteration:', it , 'loglikelihood:_', llik[it],"\n")
cat ('iteration:', it , 'loglikelihood:', llik[it],"\n")
flush.console()
ifelse( it>1 & llik[it] − llik[it−1]< minchange ,
stop ( "EM_as_converged " ) , llik[it] )
pi <− colSums (z)/N
mu <− t (z) %∗% x/colSums (z)
set.seed(1234567890)
max_it <- 100 # max number of EM iterations
min_change <- 0.1 # min change in log likelihood between two consecutive EM iterations
N=1000 # number of training points
D=10 # number of dimensions
x <- matrix(nrow=N, ncol=D) # training data
true_pi <- vector(length = 3) # true mixing coefficients
true_mu <- matrix(nrow=3, ncol=D) # true conditional distributions
true_pi=c(1/3, 1/3, 1/3)
true_mu[1,]=c(0.5,0.6,0.4,0.7,0.3,0.8,0.2,0.9,0.1,1)
true_mu[2,]=c(0.5,0.4,0.6,0.3,0.7,0.2,0.8,0.1,0.9,0)
true_mu[3,]=c(0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5)
plot(true_mu[1,], type="o", col="blue", ylim=c(0,1))
points(true_mu[2,], type="o", col="red")
points(true_mu[3,], type="o", col="green")
# Producing the training data
for(n in 1:N) {
k <- sample(1:3,1,prob=true_pi)
for(d in 1:D) {
x[n,d] <- rbinom(1,1,true_mu[k,d])
}
}
K=3 # number of guessed components
z <- matrix(nrow=N, ncol=K) # fractional component assignments
pi <- vector(length = K) # mixing coefficients
mu <- matrix(nrow=K, ncol=D) # conditional distributions
llik <- vector(length = max_it) # log likelihood of the EM iterations
# Random initialization of the paramters
pi <- runif(K,0.49,0.51)
pi <- pi / sum(pi)
for(k in 1:K) {
mu[k,] <- runif(D,0.49,0.51)
}
pi
mu
for(it in 1:max_it) {
plot(mu[1,], type="o", col="blue", ylim=c(0,1))
points(mu[2,], type="o", col="red")
points(mu[3,], type="o", col="green")
#points(mu[4,], type="o", col="yellow")
Sys.sleep(0.5)
# E−s t e p : Computat ion o f t h e f r a c t i o n a l component a s s ig nme n t s
bp <- exp(x %∗% log ( t (mu) ) + (1-x ) %∗% log (1-t (mu) ) )
pm <- matrix( rep( pi ,N) , nrow=N, ncol=K)
px <- bp∗ pm
z <- px / rowSums ( px )
#Log l i k e l i h o o d compu ta t ion .
lhood <− x %∗% log ( t (mu) ) + (1−x ) %∗% log(1−t (mu) )
for (n in 1 :N){
for ( k in 1 :K){
llik[it] <− llik [it] + z [n,k] ∗ lhood [n,k] + log (pi[k])
}
}
cat ('iteration:', it , 'loglikelihood:', llik[it],"\n")
flush.console()
ifelse( it>1 & llik[it] − llik[it−1]< minchange ,
stop ( "EM_as_converged " ) , llik[it] )
#M−s t e p : ML parame ter e s t im a t i o n from t h e d a t a and f r a c t i o n a l
component a s sig nm e n t s
pi <− colSums (z)/N
mu <− t (z) %∗% x/colSums (z)
}
pi
mu
p lot ( l l i k [ 1 : i t ] , type=”o” )
set.seed(1234567890)
max_it <- 100 # max number of EM iterations
min_change <- 0.1 # min change in log likelihood between two consecutive EM iterations
N=1000 # number of training points
D=10 # number of dimensions
x <- matrix(nrow=N, ncol=D) # training data
true_pi <- vector(length = 3) # true mixing coefficients
true_mu <- matrix(nrow=3, ncol=D) # true conditional distributions
true_pi=c(1/3, 1/3, 1/3)
true_mu[1,]=c(0.5,0.6,0.4,0.7,0.3,0.8,0.2,0.9,0.1,1)
true_mu[2,]=c(0.5,0.4,0.6,0.3,0.7,0.2,0.8,0.1,0.9,0)
true_mu[3,]=c(0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5)
plot(true_mu[1,], type="o", col="blue", ylim=c(0,1))
points(true_mu[2,], type="o", col="red")
points(true_mu[3,], type="o", col="green")
# Producing the training data
for(n in 1:N) {
k <- sample(1:3,1,prob=true_pi)
for(d in 1:D) {
x[n,d] <- rbinom(1,1,true_mu[k,d])
}
}
K=3 # number of guessed components
z <- matrix(nrow=N, ncol=K) # fractional component assignments
pi <- vector(length = K) # mixing coefficients
mu <- matrix(nrow=K, ncol=D) # conditional distributions
llik <- vector(length = max_it) # log likelihood of the EM iterations
# Random initialization of the paramters
pi <- runif(K,0.49,0.51)
pi <- pi / sum(pi)
for(k in 1:K) {
mu[k,] <- runif(D,0.49,0.51)
}
pi
mu
for(it in 1:max_it) {
plot(mu[1,], type="o", col="blue", ylim=c(0,1))
points(mu[2,], type="o", col="red")
points(mu[3,], type="o", col="green")
#points(mu[4,], type="o", col="yellow")
Sys.sleep(0.5)
# E−s t e p : Computat ion o f t h e f r a c t i o n a l component a s s ig nme n t s
bp <- exp(x %∗% log ( t (mu) ) + (1-x ) %∗% log (1-t (mu) ) )
pm <- matrix( rep( pi ,N) , nrow=N, ncol=K)
px <- bp∗ pm
z <- px / rowSums ( px )
#Log l i k e l i h o o d compu ta t ion .
lhood <− x %∗% log ( t (mu) ) + (1−x ) %∗% log(1−t (mu) )
for (n in 1 :N){
for ( k in 1 :K){
llik[it] <− llik [it] + z [n,k] ∗ lhood [n,k] + log (pi[k])
}
}
cat ('iteration:', it , 'loglikelihood:', llik[it],"\n")
flush.console()
ifelse( it>1 & llik[it] − llik[it−1]< minchange,
stop ( "EM is converged " ) , llik[it] )
# M−s t e p : ML parame ter e s t im a t i o n from t h e d a t a and
#  f r a c t i o n a lcomponent a s sig nm e n t s
pi <− colSums (z)/N
mu <− t (z) %∗% x/colSums (z)
}
pi
mu
p lot ( l l i k [ 1 : i t ] , type=”o” )
