#misscalculation
calc_class_err = function(actual, predicted) {
mean(actual != predicted)
}
calc_class_err(validation$X0.26,pred.knn_v)
table(train$X0.26,as.factor(pred.knn)) # for train data
table(test$X0.26,as.factor(pred.knn_t)) # for test data
missclassrate=function(y,y_i)
{
n=length(y)
v<-1-(sum(diag(table(y,y_i)))/n)
return(v)
}
missclassrate(train$X0.26,as.factor(pred.knn)) # for training data
missclassrate(test$X0.26,as.factor(pred.knn_t)) # for test data
# Accuracy of the data
acc=function(x,y)
{
n=length(x)
ac=sum(diag(table(x,y)))/n
return(ac)
}
acc(test$X0.26,as.factor(pred.knn_t))
#knn.fit$prob
v=data.frame(knn.fit$prob)
head(v)
estm_pb <- colnames(v)[apply(v, 1, which.max)]
v$y<-train$X0.26
head(v)
v$fit <- knn.fit$fitted
v$estm_pb <- estm_pb
###
y_8 <- v[v$y == 8,]
yhat_8 <- y_8[y_8$fit == 8,]
###
# Best
easy <- as.numeric(row.names(yhat_8[order(-yhat_8[,9]),][1:2,]))
easy
# Worse
tougher <- as.numeric(row.names(yhat_8[order(yhat_8[,9]),][1:3,]))
tougher
#best
col=heat.colors(12)
heatmap(t(matrix(unlist(train[easy[1],-65]), nrow=8)),Colv = NA, Rowv = NA,col=rev(heat.colors(12)))
heatmap(t(matrix(unlist(train[easy[2],-65]), nrow=8)), Colv = NA, Rowv = NA,col=rev(heat.colors(12)))
heatmap(t(matrix(unlist(train[tougher[1],-65]), nrow=8)), Colv = NA, Rowv = NA,col=rev(heat.colors(12)))
heatmap(t(matrix(unlist(train[tougher[2],-65]), nrow=8)), Colv = NA, Rowv = NA,col=rev(heat.colors(12)))
heatmap(t(matrix(unlist(train[tougher[3],-65]), nrow=8)), Colv = NA, Rowv = NA,col=rev(heat.colors(12)))
k=1
k.optm=c()
y.optm=c()
for (i in 1:30){
knn.fit <-kknn(as.factor(X0.26)~., train=train,test=train, k = i,kernel = "rectangular")
knn.fit_v <-kknn(as.factor(X0.26)~., train=train,test=validation, k = i,kernel = "rectangular")
ypred = fitted(knn.fit)
vpred = fitted(knn.fit_v)
k.optm[i] = 1-(sum(diag(as.matrix(table(Actual = train$X0.26, Predicted = ypred))))/nrow(train))
y.optm[i] = 1-(sum(diag(as.matrix(table(Actual = validation$X0.26, Predicted = vpred))))/nrow(validation))
}
k.optm
y.optm
my.df  <- data.frame(K_Value = c(1:30), Training= c(k.optm), Validation = c(y.optm))
plot3<-ggplot( ) +
geom_line(aes(x=my.df$K_Value,y=my.df$Validation,colour="green")) +
geom_line(aes(x=my.df$K_Value,y=my.df$Training,colour="red"))+
ylab("Missclassification Rate ") +xlab("K_value")+
scale_color_manual(name = "Missclassification Rate", labels = c("Validation ", "Training "),
values =c("green", "red"))
print(plot3)
library(ggplot2)
plot3<-ggplot( ) +
geom_line(aes(x=my.df$K_Value,y=my.df$Validation,colour="green")) +
geom_line(aes(x=my.df$K_Value,y=my.df$Training,colour="red"))+
ylab("Missclassification Rate ") +xlab("K_value")+
scale_color_manual(name = "Missclassification Rate", labels = c("Validation ", "Training "),
values =c("green", "red"))
print(plot3)
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(glmnet)
library(psych)
library(tidyr)
library(kknn)
#Load Data
dataset = read.csv("C:/Users/Mounish/Desktop/ML/Lab1 Block 1/optdigits.csv")
cat("Dimension of Dataset is",dim(dataset),"\n")
#Split the data into train, validation and test
n <- dim(dataset)[1]
set.seed(12345)
id <- sample(1:n, floor(n*0.5))
train <- dataset[id,]
id1 <- setdiff(1:n, id)
set.seed(12345)
id2 <- sample(id1, floor(n*0.25))
validation<- dataset[id2,]
id3 <- setdiff(id1,id2)
test <- dataset[id3,]
knn.fit <- kknn(as.factor(X0.26)~., train=train,test=train ,k = 30,
kernel = "rectangular")
knn.fit_v <- kknn(as.factor(X0.26)~., train=validation,test=validation,k = 30,
kernel = "rectangular")
knn.fit_t <- kknn(as.factor(X0.26)~., train=test,test=test,k = 30,
kernel = "rectangular")
pred.knn <- fitted(knn.fit)
pred.knn_v <- fitted(knn.fit_v)
pred.knn_t <- fitted(knn.fit_t)
c<-table(train$X0.26,as.factor(pred.knn)) # for train data
cat("Confusion matrix for the training data.")
print(c)
v<-table(test$X0.26,as.factor(pred.knn_t)) # for test data
cat("Confusion matrix for the test data.")
print(v)
c<-table(train$X0.26,as.factor(pred.knn)) # for train data
cat("Confusion matrix for the training data.")
print(c)
v<-table(test$X0.26,as.factor(pred.knn_t)) # for test data
cat("Confusion matrix for the test data.")
print(v)
missclassrate=function(y,y_i)
{
n=length(y)
v<-1-(sum(diag(table(y,y_i)))/n)
return(v)
}
missclassrate(train$X0.26,as.factor(pred.knn)) # for training data
missclassrate(test$X0.26,as.factor(pred.knn_t)) # for test data
v=data.frame(knn.fit$prob)
estm_pb <- colnames(v)[apply(v, 1, which.max)]
v$y<-train$X0.26
v$fit <- knn.fit$fitted
v$estm_pb <- estm_pb
###
y_8 <- v[v$y == 8,]
yhat_8 <- y_8[y_8$fit == 8,]
###
# Easy
easy <- as.numeric(row.names(yhat_8[order(-yhat_8[,9]),][1:2,]))
# Tough
tougher <- as.numeric(row.names(yhat_8[order(yhat_8[,9]),][1:3,]))
col=heat.colors(12)
heatmap(t(matrix(unlist(train[easy[1],-65]), nrow=8)),Colv = NA, Rowv = NA,col=rev(heat.colors(12)))
heatmap(t(matrix(unlist(train[easy[2],-65]), nrow=8)), Colv = NA, Rowv = NA,col=rev(heat.colors(12)))
heatmap(t(matrix(unlist(train[tougher[1],-65]), nrow=8)), Colv = NA, Rowv = NA,col=rev(heat.colors(12)))
heatmap(t(matrix(unlist(train[tougher[2],-65]), nrow=8)), Colv = NA, Rowv = NA,col=rev(heat.colors(12)))
heatmap(t(matrix(unlist(train[tougher[3],-65]), nrow=8)), Colv = NA, Rowv = NA,col=rev(heat.colors(12)))
View(dataset)
#setwd("C:/Users/Mounish/Desktop/ML/lab1")
dataset = read.csv("C:/Users/Mounish/Desktop/ML/Lab1 Block 1/optdigits.csv")
dim(dataset)
####
n <- dim(dataset)[1]
set.seed(12345)
id <- sample(1:n, floor(n*0.5))
train <- dataset[id,]
id1 <- setdiff(1:n, id)
set.seed(12345)
id2 <- sample(id1, floor(n*0.25))
validation<- dataset[id2,]
id3 <- setdiff(id1,id2)
test <- dataset[id3,]
####
dim(train)
dim(test)
dim(validation)
library(kknn)
knn.fit <- kknn(as.factor(X0.26)~., train=train,test=train ,k = 30,
kernel = "rectangular")
knn.fit_v <- kknn(as.factor(X0.26)~., train=validation,test=validation,k = 30,
kernel = "rectangular")
knn.fit_t <- kknn(as.factor(X0.26)~., train=test,test=test,k = 30,
kernel = "rectangular")
pred.knn <- fitted(knn.fit)
pred.knn_v <- fitted(knn.fit_v)
pred.knn_t <- fitted(knn.fit_t)
cm=as.matrix(table(actual=validation$X0.26, Predicted = pred.knn_v))
accuracy=sum(diag(cm))/length(validation$X0.26)
accuracy
#misscalculation
calc_class_err = function(actual, predicted) {
mean(actual != predicted)
}
calc_class_err(validation$X0.26,pred.knn_v)
table(train$X0.26,as.factor(pred.knn)) # for train data
table(test$X0.26,as.factor(pred.knn_t)) # for test data
missclassrate=function(y,y_i)
{
n=length(y)
v<-1-(sum(diag(table(y,y_i)))/n)
return(v)
}
missclassrate(train$X0.26,as.factor(pred.knn)) # for training data
missclassrate(test$X0.26,as.factor(pred.knn_t)) # for test data
# Accuracy of the data
acc=function(x,y)
{
n=length(x)
ac=sum(diag(table(x,y)))/n
return(ac)
}
acc(test$X0.26,as.factor(pred.knn_t))
#knn.fit$prob
v=data.frame(knn.fit$prob)
head(v)
head(v)
estm_pb <- colnames(v)[apply(v, 1, which.max)]
v$y<-train$X0.26
head(v)
v$fit <- knn.fit$fitted
v$estm_pb <- estm_pb
###
y_8 <- v[v$y == 8,]
yhat_8 <- y_8[y_8$fit == 8,]
###
# Best
easy <- as.numeric(row.names(yhat_8[order(-yhat_8[,9]),][1:2,]))
easy
# Worse
tougher <- as.numeric(row.names(yhat_8[order(yhat_8[,9]),][1:3,]))
tougher
#best
col=heat.colors(12)
heatmap(t(matrix(unlist(train[easy[1],-65]), nrow=8)),Colv = NA, Rowv = NA,col=rev(heat.colors(12)))
heatmap(t(matrix(unlist(train[easy[2],-65]), nrow=8)), Colv = NA, Rowv = NA,col=rev(heat.colors(12)))
heatmap(t(matrix(unlist(train[tougher[1],-65]), nrow=8)), Colv = NA, Rowv = NA,col=rev(heat.colors(12)))
heatmap(t(matrix(unlist(train[tougher[2],-65]), nrow=8)), Colv = NA, Rowv = NA,col=rev(heat.colors(12)))
heatmap(t(matrix(unlist(train[tougher[3],-65]), nrow=8)), Colv = NA, Rowv = NA,col=rev(heat.colors(12)))
dataset = read.csv("C:/Users/Mounish/Desktop/ML/Lab1 Block 1/parkinsons.csv")
View(dataset)
p<-dataset$motor_UPDRS
plot(p)
hist(p)
#Q2
n <- dim(dataset)[1]
n
id <- sample(1:n, floor(n*0.6))
train <- dataset[id,]
id1 <- setdiff(1:n, id)
histogram
hist
?hist
dataset = read.csv("C:/Users/Mounish/Desktop/ML/Lab1 Block 1/parkinsons.csv")
# Q1
p<-dataset$motor_UPDRS
hist(p,main=Distribution of Y_hat)
n <- dim(dataset)[1]
dataset = read.csv("C:/Users/Mounish/Desktop/ML/Lab1 Block 1/parkinsons.csv")
# Q1
p<-dataset$motor_UPDRS
hist(p,main="Distribution of Y_hat")
n <- dim(dataset)[1]
set.seed(12345)
id <- sample(1:n, floor(n*0.6))  #Training split
train <- dataset[id,]
id1 <- setdiff(1:n, id)         # Setting different data b/w Trainig and test
set.seed(12345)
id2 <- sample(id1, floor(n*0.4))
test<- dataset[id2,]
dim(train)
dim(test)
5875*0.6
dataset = read.csv("C:/Users/Mounish/Desktop/ML/Lab1 Block 1/parkinsons.csv")
# Q1
p<-dataset$motor_UPDRS
hist(p,main="Distribution of Y_hat")
n <- dim(dataset)[1]
set.seed(12345)
id <- sample(1:n, floor(n*0.6))  #Training split
train <- dataset[id,]
id1 <- setdiff(1:n, id)         # Setting different data b/w Trainig and test
set.seed(12345)
id2 <- sample(id1, floor(n*0.4))
test<- dataset[id2,]
dim(train)
dim(test)
View(dataset)
View(dataset)
loglik<-function(x,y,w,sigma){
loglik<-function(x,y,w,sigma){
dataset = read.csv("C:/Users/Mounish/Desktop/ML/Lab1 Block 1/parkinsons.csv")
# Q1
p<-dataset$motor_UPDRS
hist(p,main="Distribution of Y_hat")
n <- dim(dataset)[1]
set.seed(12345)
id <- sample(1:n, floor(n*0.6))  #Training split
train <- dataset[id,]
id1 <- setdiff(1:n, id)         # Setting different data b/w Trainig and test
set.seed(12345)
id2 <- sample(id1, floor(n*0.4))
test<- dataset[id2,]
dim(train)
dim(test)
loglik<-function(x,y,w,sigma){
f=-sum((y - t(t(w)%*%t(x)))**2) / (2*(sigma**2))
s=n*log(1/sqrt(2*pi*(sigma**2))
ans=f+s
return(ans)
}
loglik<-function(x,y,w,sigma){
n=nrow(x)
f=-sum((y - t(t(w)%*%t(x)))**2) / (2*(sigma**2))
s=n*log(1/sqrt(2*pi*(sigma**2)))
ans=f+s
return(ans)
}
dataset[17]
dataset = read.csv("C:/Users/Mounish/Desktop/ML/Lab1 Block 1/parkinsons.csv")
# Q1
p<-dataset$motor_UPDRS
hist(p,main="Distribution of Y_hat")
n <- dim(dataset)[1]
set.seed(12345)
id <- sample(1:n, floor(n*0.6))  #Training split
train <- dataset[id,]
id1 <- setdiff(1:n, id)         # Setting different data b/w Trainig and test
set.seed(12345)
id2 <- sample(id1, floor(n*0.4))
test<- dataset[id2,]
dim(train)
dim(test)
# 3a
#Loglikelihood function
loglik<-function(x,y,w,sigma){
n=nrow(x)
f=-sum((y - t(t(w)%*%t(x)))**2) / (2*(sigma**2))
s=n*log(1/sqrt(2*pi*(sigma**2)))
ans=f+s
return(ans)
}
loglik()
dataset[17]
dataset[17]
dataset[,17]
#libraries
library(ggplot2)
library(glmnet)
library(psych)
library(tidyr)
# Read and split the data
data = read.csv("C:/Users/Mounish/Desktop/ML/Lab1 Block 1/tecator.csv")
# Split data into train and test set
n = dim(data)[1]
set.seed(12345)
id = sample(1:n, floor(n*0.5))
train = data[id,]
test = data[-id,]
fit<-lm(Fat~ .-ï..Sample-Protein-Moisture,data=train)
summary(fit)
lm_train_pred = predict(fit, train)
train_mse = mean((train$Fat-lm_train_pred)^2)
train_rmse = sqrt(mean((train$Fat-lm_train_pred)^2))
test_predictions_lm = predict(fit, test)
test_mse = mean((test$Fat-test_predictions_lm)^2)
test_rmse = sqrt(mean((test$Fat-test_predictions_lm)^2))
cat("Train error\nMSE: ", train_mse, "\nRMSE: ", train_rmse,"\n\nTest error\nMSE: ", test_mse, "\nRMSE: ", test_rmse)
covariates = train[,2:101]
response = train$Fat
set.seed(12345)
model_lasso = glmnet(as.matrix(covariates), response, alpha=1, family="gaussian")
plot(model_lasso, xvar="lambda", label=T)
plot(model_lasso$lambda,model_lasso$df,col="red",
ylab="Degrees of Freedom",
xlab="Lambda")
plot(fit)
#libraries
library(ggplot2)
library(glmnet)
library(psych)
library(tidyr)
# Read and split the data
data = read.csv("C:/Users/Mounish/Desktop/ML/Lab1 Block 1/tecator.csv")
# Split data into train and test set
n = dim(data)[1]
set.seed(12345)
id = sample(1:n, floor(n*0.5))
train = data[id,]
test = data[-id,]
fit<-lm(Fat~ .-ï..Sample-Protein-Moisture,data=train)
summary(fit)
lm_train_pred = predict(fit, train)
train_mse = mean((train$Fat-lm_train_pred)^2)
train_rmse = sqrt(mean((train$Fat-lm_train_pred)^2))
test_predictions_lm = predict(fit, test)
test_mse = mean((test$Fat-test_predictions_lm)^2)
test_rmse = sqrt(mean((test$Fat-test_predictions_lm)^2))
cat("Train error\nMSE: ", train_mse, "\nRMSE: ", train_rmse,"\n\nTest error\nMSE: ", test_mse, "\nRMSE: ", test_rmse)
covariates = train[,2:101]
response = train$Fat
set.seed(12345)
model_lasso = glmnet(as.matrix(covariates), response, alpha=1, family="gaussian")
plot(model_lasso, xvar="lambda", label=T)
plot(model_lasso$lambda,model_lasso$df,col="red",
ylab="Degrees of Freedom",
xlab="Lambda")
model_ridge = glmnet(as.matrix(covariates), response, alpha=0, family="gaussian")
plot(model_ridge, xvar="lambda", label=T)
10^-1
10^-0.3
covariates = scale( train[,2:101])
response = scale(train$Fat)
#libraries
library(ggplot2)
library(glmnet)
library(psych)
library(tidyr)
# Read and split the data
data = read.csv("C:/Users/Mounish/Desktop/ML/Lab1 Block 1/tecator.csv")
# Split data into train and test set
n = dim(data)[1]
set.seed(12345)
id = sample(1:n, floor(n*0.5))
train = data[id,]
test = data[-id,]
fit<-lm(Fat~ .-ï..Sample-Protein-Moisture,data=train)
summary(fit)
lm_train_pred = predict(fit, train)
train_mse = mean((train$Fat-lm_train_pred)^2)
train_rmse = sqrt(mean((train$Fat-lm_train_pred)^2))
test_predictions_lm = predict(fit, test)
test_mse = mean((test$Fat-test_predictions_lm)^2)
test_rmse = sqrt(mean((test$Fat-test_predictions_lm)^2))
cat("Train error\nMSE: ", train_mse, "\nRMSE: ", train_rmse,"\n\nTest error\nMSE: ", test_mse, "\nRMSE: ", test_rmse)
covariates = scale( train[,2:101])
response = scale(train$Fat)
set.seed(12345)
model_lasso = glmnet(as.matrix(covariates), response, alpha=1, family="gaussian")
plot(model_lasso, xvar="lambda", label=T)
10^
#================================================================================================================
# 4.Presenting a plot that represent how DOF depend on the penalty parameter.
plot(model_lasso$lambda,model_lasso$df,col="red",
ylab="Degrees of Freedom",
xlab="Lambda")
model_ridge = glmnet(as.matrix(covariates), response, alpha=0, family="gaussian")
plot(model_ridge, xvar="lambda", label=T)
cv_model1<-cv.glmnet(as.matrix(x_train),y_train,family="gaussian" , lambda=10^seq(-5,5,length=500), alpha=1)
cv_model1<-cv.glmnet(as.matrix(covariates),response,family="gaussian" , lambda=10^seq(-5,5,length=500), alpha=1)
plot(cv_model1)
lambda_optimal<-cv_model1$lambda.1se
lambda_optimal
plot(lambda_optimal)
?lambda.1se
lambda_optimal<-cv_model1$lambda
lambda_optimal
lambda_optimal<-cv_model1$lambda.1se
lambda_optimal
plot(cv_model1)
lambda_optimal<-cv_model1$lambda.1se
lambda_optimal
features<-as.matrix(coef(cv_model1,cv_model1$lambda.1se))
optimal_features<-features[features!=0,]
length(optimal_features)
features
coef(cv_model1)
length(coef(cv_model1))
features<-as.matrix(coef(cv_model1))
optimal_features<-features[features!=0,]
length(optimal_features)
length(coef(cv_model1))
#libraries
library(ggplot2)
library(glmnet)
library(psych)
library(tidyr)
# Read and split the data
data = read.csv("C:/Users/Mounish/Desktop/ML/Lab1 Block 1/tecator.csv")
# Split data into train and test set
n = dim(data)[1]
set.seed(12345)
id = sample(1:n, floor(n*0.5))
train = data[id,]
test = data[-id,]
#=================================================================================================================
# 1.fit the linear model to the this data
fit<-lm(Fat~ .-ï..Sample-Protein-Moisture,data=train)
summary(fit)
lm_train_pred = predict(fit, train)
train_mse = mean((train$Fat-lm_train_pred)^2)
train_rmse = sqrt(mean((train$Fat-lm_train_pred)^2))
test_predictions_lm = predict(fit, test)
test_mse = mean((test$Fat-test_predictions_lm)^2)
test_rmse = sqrt(mean((test$Fat-test_predictions_lm)^2))
cat("Train error\nMSE: ", train_mse, "\nRMSE: ", train_rmse,"\n\nTest error\nMSE: ", test_mse, "\nRMSE: ", test_rmse)
#================================================================================================================
# 3.Fit The Lasso model
covariates = scale( train[,2:101])
response = scale(train$Fat)
set.seed(12345)
model_lasso = glmnet(as.matrix(covariates), response, alpha=1, family="gaussian")
plot(model_lasso, xvar="lambda", label=T)
10^
#================================================================================================================
# 4.Presenting a plot that represent how DOF depend on the penalty parameter.
plot(model_lasso$lambda,model_lasso$df,col="red",
ylab="Degrees of Freedom",
xlab="Lambda")
#By the decreasing degrees of freedom , the lambda value of the model get decreased, but the
#=================================================================================================================
#5.Fit the ridge model.
model_ridge = glmnet(as.matrix(covariates), response, alpha=0, family="gaussian")
plot(model_ridge, xvar="lambda", label=T)
#=========================================================================================================================
# 6.Optimal Lasso
set.seed(12345)
cv_model1<-cv.glmnet(as.matrix(covariates),response,family="gaussian" , lambda=10^seq(-5,5,length=500), alpha=1)
plot(cv_model1)
lambda_optimal<-cv_model1$lambda.1se
lambda_optimal
#Finding the optimal features
features<-as.matrix(coef(cv_model1))
optimal_features<-features[features!=0,]
length(optimal_features)
